{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"DLCV_HW3_P4_ADDA.ipynb","provenance":[{"file_id":"1oWSfUKxJHKyxopTTxbGZmUkdnA2aDiEL","timestamp":1606395818010},{"file_id":"18AuikHvnIs4hiEgMdbGd_KkcPgZqZoRA","timestamp":1606327383683},{"file_id":"1s5nNaet13a1Wv5VKHZG5yIM-yXCqav4x","timestamp":1606013215166},{"file_id":"13et4UUFeGBXhMKRFSEEdDq4ecFkWspb4","timestamp":1605863874567}],"collapsed_sections":["RRyZg753Z-Ns"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"jZPETjG0TtJw","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1606474438970,"user_tz":-480,"elapsed":14892,"user":{"displayName":"曹林熹","photoUrl":"","userId":"17830086974482193998"}},"outputId":"346f18d0-a890-41a5-8f0e-71e41bc3de23"},"source":["# connect to drive\n","from google.colab import drive\n","drive.mount(\"/content/drive\", force_remount=True)\n","path = 'drive/My Drive/senior_1/DLCV/HW/hw3'"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"jE8BgmVGBflm"},"source":["!gdown --id '1Cz5eLSP7QRMkO0PqxZLldJ36nK6EWHR8' --output hw3_data.zip # 下載資料集\n","!unzip hw3_data.zip # 解壓縮\n","'''\n","1.  hw3_data/digits/mnistm/\n","      # num of data: 60,000 (training) / 10,000 (testing) # num of classes: 10 (0~9) # Image size: 28*28*3\n","      train/images.png\n","      test/images.png\n","      train.csv\n","      test.csv\n","2.  hw3_data/digits/svhn/\n","      # num of data: 73,257 (training) / 26,032 (testing) # num of classes: 10 (0~9) # Image size: 28*28*3\n","      train/images.png\n","      test/images.png\n","      train.csv\n","      test.csv\n","3.  hw3_data/digits/usps/\n","      # num of data: 7,291 (training) / 2,007 (testing) # num of classes: 10 (0~9) # Image size: 28*28*1\n","      train/images.png  \n","      test/images.png  \n","      train.csv\n","      test.csv\n","'''"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EoH4FXqgdzT_"},"source":["# set packages\n","!pip3 install certifi==2020.6.20\n","!pip3 install cycler==0.10.0\n","!pip3 install joblib==0.17.0\n","!pip3 install kiwisolver==1.2.0\n","!pip3 install matplotlib==3.3.2\n","!pip3 install numpy==1.18.1\n","!pip3 install pandas==1.1.3\n","!pip3 install Pillow==8.0.0\n","!pip3 install pyparsing==2.4.7\n","!pip3 install python-dateutil==2.8.1\n","!pip3 install pytz==2020.1\n","!pip3 install scikit-learn==0.21.3\n","!pip3 install scipy==1.2.1\n","!pip3 install six==1.15.0\n","!pip3 install torch==1.4.0\n","!pip3 install torchvision==0.5.0"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"T_aIPbf79IDp","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1606474440508,"user_tz":-480,"elapsed":5255,"user":{"displayName":"曹林熹","photoUrl":"","userId":"17830086974482193998"}},"outputId":"257309d9-9589-4f85-e16e-74cd3bf851d4"},"source":["# Import 需要的套件\n","import os\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torchvision.models as models\n","import torchvision\n","from torch.utils.data import DataLoader, Dataset\n","import torchvision.transforms as transforms\n","import time\n","import pandas as pd\n","import random\n","import scipy.misc\n","import argparse\n","import imageio\n","from torch.autograd import Variable\n","from torch.optim import Adam, AdamW\n","import csv\n","import matplotlib.pyplot as plt\n","from sklearn.model_selection import train_test_split\n","from sklearn import manifold\n","from torch.autograd import Function\n","import torch.optim as optim\n","\n","# 固定隨機種子\n","def same_seeds(seed):\n","    torch.manual_seed(seed)\n","    if torch.cuda.is_available():\n","        torch.cuda.manual_seed(seed)\n","        torch.cuda.manual_seed_all(seed)  # if you are using multi-GPU.\n","    np.random.seed(seed)  # Numpy module.\n","    random.seed(seed)  # Python random module.\n","    torch.backends.cudnn.benchmark = False\n","    torch.backends.cudnn.deterministic = True\n","    print(\"torchvision.__version__ =\", torchvision.__version__)\n","    print(\"torch.cuda.is_available() =\", torch.cuda.is_available())\n","\n","same_seeds(0)"],"execution_count":2,"outputs":[{"output_type":"stream","text":["torchvision.__version__ = 0.5.0\n","torch.cuda.is_available() = True\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"S6gYyGuz98lI"},"source":["## Dataset"]},{"cell_type":"code","metadata":{"id":"x3L53lwYCFUv","executionInfo":{"status":"ok","timestamp":1606474442069,"user_tz":-480,"elapsed":855,"user":{"displayName":"曹林熹","photoUrl":"","userId":"17830086974482193998"}}},"source":["def sortfile(path):\n","    index = []\n","    image_dir = sorted(os.listdir(path)) # 把圖檔按照編號排列\n","    for i, file in enumerate(image_dir):\n","        index.append([file, int(file.replace('.png', ''))])\n","    index = sorted(index, key = lambda s: s[1])\n","    return index\n","\n","def readfile(path, index, mode):\n","    x = np.zeros((len(index), 28, 28, 3), dtype=np.uint8)\n","    y = np.zeros((len(index)), dtype=np.uint8)\n","    pd_y = pd.read_csv(os.path.join(path, mode + \".csv\"))\n","    for i, file in enumerate(index):\n","        img = imageio.imread(os.path.join(os.path.join(path, mode), file[0]))\n","        if 'usps' in path:\n","            temp_x = img.reshape(28, 28, 1) # expand dim\n","            x[i, :, :] = np.concatenate((temp_x, temp_x, temp_x), axis = 2)\n","        else:\n","            x[i, :, :] = img \n","        y[i] = pd_y['label'][i] \n","    return x, y"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sUBQixrwq0Iv","executionInfo":{"status":"ok","timestamp":1606474648252,"user_tz":-480,"elapsed":58770,"user":{"displayName":"曹林熹","photoUrl":"","userId":"17830086974482193998"}},"outputId":"20b9ca7b-0182-4c64-f043-01d4f83b7abc"},"source":["# 分別將 training set、testing set 用 sortfile, readfile 函式讀進來\n","workspace_dir = './hw3_data/digits'\n","digits_data_list = ['mnistm', 'svhn', 'usps']\n","\n","for index in digits_data_list:\n","    print(\"Reading \" + index + \" data\")\n","    split_ratio = 0.2\n","    train_index = sortfile(os.path.join(workspace_dir, os.path.join(index, \"train\")))\n","    test_index = sortfile(os.path.join(workspace_dir, os.path.join(index, \"test\")))\n","    if index == 'mnistm':\n","        mnistm_train_x, mnistm_train_y = readfile(os.path.join(workspace_dir, index), train_index, \"train\")\n","        # split train / val, ratio = split_ratio\n","        mnistm_train_x, mnistm_val_x, mnistm_train_y, mnistm_val_y = train_test_split(mnistm_train_x, mnistm_train_y, test_size = split_ratio, random_state = 3)\n","        mnistm_test_x, mnistm_test_y = readfile(os.path.join(workspace_dir, index), test_index, \"test\")\n","        mnistm_test_index = pd.DataFrame(test_index)[0].values.tolist()\n","    elif index == 'svhn':\n","        svhn_train_x, svhn_train_y = readfile(os.path.join(workspace_dir, index), train_index, \"train\")\n","        # split train / val, ratio = split_ratio\n","        svhn_train_x, svhn_val_x, svhn_train_y, svhn_val_y = train_test_split(svhn_train_x, svhn_train_y, test_size = split_ratio, random_state = 3) \n","        svhn_test_x, svhn_test_y = readfile(os.path.join(workspace_dir, index), test_index, \"test\")\n","        svhn_test_index = pd.DataFrame(test_index)[0].values.tolist()  \n","    else:\n","        usps_train_x, usps_train_y = readfile(os.path.join(workspace_dir, index), train_index, \"train\")\n","        # split train / val, ratio = split_ratio\n","        usps_train_x,usps_val_x, usps_train_y, usps_val_y = train_test_split(usps_train_x, usps_train_y, test_size = split_ratio, random_state = 3)\n","        usps_test_x, usps_test_y = readfile(os.path.join(workspace_dir, index), test_index, \"test\") \n","        usps_test_index = pd.DataFrame(test_index)[0].values.tolist()\n","    print(\"Size of {} training data = {}\".format(index, round(len(train_index)*(1-split_ratio))))\n","    print(\"Size of {} validation data = {}\".format(index, round(len(train_index)*split_ratio)))\n","    print(\"Size of {} testing data = {}\".format(index, round(len(test_index))))"],"execution_count":5,"outputs":[{"output_type":"stream","text":["Reading mnistm data\n","Size of mnistm training data = 48000\n","Size of mnistm validation data = 12000\n","Size of mnistm testing data = 10000\n","Reading svhn data\n","Size of svhn training data = 58606\n","Size of svhn validation data = 14651\n","Size of svhn testing data = 26032\n","Reading usps data\n","Size of usps training data = 5833\n","Size of usps validation data = 1458\n","Size of usps testing data = 2007\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"JbqRpnv2q10H","executionInfo":{"status":"ok","timestamp":1606474660582,"user_tz":-480,"elapsed":895,"user":{"displayName":"曹林熹","photoUrl":"","userId":"17830086974482193998"}}},"source":["# training 時需做 data augmentation\n","train_transform = transforms.Compose([\n","    transforms.ToPILImage(), # 轉成 python 圖片\n","    transforms.RandomHorizontalFlip(), # 隨機將圖片水平翻轉\n","    transforms.RandomRotation(15), # 隨機旋轉圖片，表示在（-15，+15）之間隨機旋轉，旋轉後空的地方補 0\n","    transforms.ToTensor(), # 將圖片轉成 Tensor，並把數值 normalize 到 [0,1] (data normalization) ps. Tensor 為多維張量\n","    transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)) # normalize\n","])\n","\n","# testing 時不需做 data augmentation\n","test_transform = transforms.Compose([\n","    transforms.ToPILImage(),                                    \n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)) # normalize\n","])\n","\n","class ImgDataset(Dataset):\n","    def __init__(self, x, y=None, transform=None): # transform 自己決定\n","        self.x = x\n","        # label is required to be a LongTensor\n","        self.y = y\n","        if y is not None:\n","            self.y = torch.LongTensor(y)\n","        self.transform = transform\n","    def __len__(self):\n","        return len(self.x)\n","    def __getitem__(self, index):\n","        X = self.x[index]\n","        if self.transform is not None:\n","            X = self.transform(X)\n","        if self.y is not None:\n","            Y = self.y[index]\n","            return X, Y\n","        else:\n","            return X"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Iq4SJRmbq3id","executionInfo":{"status":"ok","timestamp":1606474662221,"user_tz":-480,"elapsed":766,"user":{"displayName":"曹林熹","photoUrl":"","userId":"17830086974482193998"}},"outputId":"0d2720b9-61b4-4d1a-cdaa-f465d6a78ec0"},"source":["batch_size = 64\n","train_x_list = [mnistm_train_x, svhn_train_x, usps_train_x]\n","val_x_list = [mnistm_val_x, svhn_val_x, usps_val_x]\n","test_x_list = [mnistm_test_x, svhn_test_x, usps_test_x]\n","train_y_list = [mnistm_train_y, svhn_train_y, usps_train_y]\n","val_y_list = [mnistm_val_y, svhn_val_y, usps_val_y]\n","test_y_list = [mnistm_test_y, svhn_test_y, usps_test_y]\n","for i in range(len(train_x_list)):\n","    train_set = ImgDataset(train_x_list[i], train_y_list[i], transform=train_transform)\n","    val_set = ImgDataset(val_x_list[i], val_y_list[i], transform=test_transform)\n","    test_set = ImgDataset(test_x_list[i], test_y_list[i], transform=test_transform)\n","    if i == 0: # mnistm\n","        mnistm_train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n","        mnistm_val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=False)\n","        mnistm_test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False)\n","        print('finish mnistm_loader')\n","    elif i == 1: # svhn\n","        svhn_train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n","        svhn_val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=False)\n","        svhn_test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False)\n","        print('finish svhn_loader')\n","    else: # usps\n","        usps_train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n","        usps_val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=False)\n","        usps_test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False)\n","        print('finish usps_loader')"],"execution_count":7,"outputs":[{"output_type":"stream","text":["finish mnistm_loader\n","finish svhn_loader\n","finish usps_loader\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"nT99i6XG-Waq"},"source":["## Model"]},{"cell_type":"code","metadata":{"id":"d7YlE67ilESb","executionInfo":{"status":"ok","timestamp":1606475915308,"user_tz":-480,"elapsed":761,"user":{"displayName":"曹林熹","photoUrl":"","userId":"17830086974482193998"}}},"source":["class USPS2MNISTM_SourceClassifier(nn.Module):\n","\n","    def __init__(self):\n","        super(USPS2MNISTM_SourceClassifier, self).__init__()\n","\n","        self.conv = nn.Sequential(\n","            # input_size = (28, 28, 3)\n","            # output_size = (input_size-kernel_size+2*padding)/stride + 1\n","            nn.Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(3, 3)), \n","            nn.BatchNorm2d(64),\n","            nn.ReLU(),\n","            # [batch_size, 64, 32, 32]\n","\n","            nn.Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n","            nn.BatchNorm2d(128),\n","            nn.ReLU(),\n","            # [batch_size, 128, 32, 32]\n","\n","            nn.Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n","            nn.BatchNorm2d(256),\n","            nn.ReLU(),\n","            nn.MaxPool2d(2, 2, 0),\n","             # [batch_size, 256, 16, 16]\n","\n","            nn.Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), \n","            nn.BatchNorm2d(512),\n","            nn.ReLU(),\n","            nn.MaxPool2d(2, 2, 0),\n","            # [batch_size, 512, 8, 8]\n","\n","            nn.Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n","            nn.BatchNorm2d(512),\n","            nn.ReLU(),\n","            nn.MaxPool2d(2, 2, 0),\n","            # [batch_size, 512, 4, 4]\n","\n","            nn.Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n","            nn.BatchNorm2d(512),\n","            nn.ReLU(),\n","            nn.MaxPool2d(2, 2, 0),\n","            # [batch_size, 512, 2, 2]      \n","        )   \n","        \n","        self.linear = nn.Sequential(\n","            # flatten\n","            nn.Linear(in_features=512*2*2, out_features=1024, bias=True),\n","            nn.ReLU(inplace=True),\n","            nn.Dropout(p=0.5, inplace=False),\n","\n","            nn.Linear(in_features=1024, out_features=512, bias=True),\n","            nn.ReLU(inplace=True),\n","            nn.Dropout(p=0.5, inplace=False),\n","\n","            nn.Linear(in_features=512, out_features=10, bias=True),\n","        )       \n","\n","    def forward(self, x):\n","        x = self.conv(x)\n","        x = x.view(x.size()[0], -1)\n","        out = self.linear(x)\n","        return x\n","\n","\"\"\"Discriminator model for ADDA.\"\"\"\n","\n","class Discriminator(nn.Module):\n","    \"\"\"Discriminator model for source domain.\"\"\"\n","\n","    def __init__(self, input_dims, hidden_dims, output_dims):\n","        \"\"\"Init discriminator.\"\"\"\n","        super(Discriminator, self).__init__()\n","        self.layer = nn.Sequential(\n","            nn.Linear(input_dims, hidden_dims),\n","            # nn.BatchNorm1d(hidden_dims),\n","            nn.ReLU(),\n","            nn.Linear(hidden_dims, hidden_dims),\n","            # nn.BatchNorm1d(hidden_dims),\n","            nn.ReLU(),\n","            nn.Linear(hidden_dims, output_dims),\n","            # nn.LogSoftmax()\n","            nn.Softmax()\n","        )\n","\n","    def forward(self, x):\n","        \"\"\"Forward the discriminator.\"\"\"\n","        out = self.layer(x)\n","        return out"],"execution_count":34,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SJ8uJQKiaAKi"},"source":["#util"]},{"cell_type":"code","metadata":{"id":"Bowo4ZpFaByR"},"source":["\"\"\"Utilities for ADDA.\"\"\"\n","def init_weights(layer):\n","    \"\"\"Init weights for layers w.r.t. the original paper.\"\"\"\n","    layer_name = layer.__class__.__name__\n","    if layer_name.find(\"Conv\") != -1:\n","        layer.weight.data.normal_(0.0, 0.02)\n","    elif layer_name.find(\"BatchNorm\") != -1:\n","        layer.weight.data.normal_(1.0, 0.02)\n","        layer.bias.data.fill_(0)\n","\n","def init_model(net, restore):\n","    \"\"\"Init models with cuda and weights.\"\"\"\n","    # init weights of model\n","    # net.apply(init_weights)\n","\n","    # restore model weights\n","    if restore is not None and os.path.exists(restore):\n","        net.load_state_dict(torch.load(restore))\n","        net.restored = True\n","        print(\"Restore model from: {}\".format(os.path.abspath(restore)))\n","\n","    return net"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":627},"id":"lCnl0Djs63ZK","executionInfo":{"status":"error","timestamp":1606476815327,"user_tz":-480,"elapsed":90464,"user":{"displayName":"曹林熹","photoUrl":"","userId":"17830086974482193998"}},"outputId":"96728c68-add3-489c-8414-f7fd69937439"},"source":["same_seeds(0)\n","num_epochs = 100\n","path = 'drive/My Drive/senior_1/DLCV/HW/hw3'\n","best_val_acc = 0.0\n","\n","# load dataset\n","src_data_loader = usps_train_loader\n","src_data_loader_eval = usps_test_loader\n","tgt_data_loader = mnistm_train_loader\n","tgt_data_loader_eval = mnistm_test_loader\n","\n","# set pretrained model\n","USPS2MNISTM_source_model = USPS2MNISTM_SourceClassifier().cuda()\n","USPS2MNISTM_source_model.load_state_dict(torch.load(os.path.join(path, 'p3_USPS2MNISTM_source_model.pkl')))\n","\n","# set model\n","src_encoder = USPS2MNISTM_source_model.conv.cuda() # LenetEncoder\n","src_classifier = USPS2MNISTM_source_model.linear.cuda() # LenetClassifier\n","tgt_encoder = USPS2MNISTM_SourceClassifier().conv.cuda()\n","critic = Discriminator(input_dims=512*2*2, hidden_dims=512, output_dims=2).cuda()\n","\n","criterion = nn.CrossEntropyLoss()\n","# criterion = nn.BCEWithLogitsLoss()\n","optimizer_tgt = optim.Adam(tgt_encoder.parameters(), lr=0.0001, betas=(0.5,0.9))\n","optimizer_critic = optim.Adam(critic.parameters(), lr=0.0001, betas=(0.5,0.9))\n","len_data_loader = min(len(src_data_loader), len(tgt_data_loader))\n","\n","for epoch in range(num_epochs):\n","    tgt_encoder.train()\n","    critic.train()\n","    domain_acc = 0.0\n","    for step, ((images_src, _), (images_tgt, _)) in enumerate(zip(src_data_loader, tgt_data_loader)):\n","        ###########################\n","        # 2.1 train discriminator #\n","        ###########################\n","        images_src = images_src.cuda()\n","        images_tgt = images_tgt.cuda()\n","\n","        # zero gradients for optimizer\n","        optimizer_critic.zero_grad()\n","\n","        # extract and concat features\n","        feat_src = src_encoder(images_src)\n","        feat_tgt = tgt_encoder(images_tgt)\n","        feat_concat = torch.cat((feat_src, feat_tgt), dim=0)\n","\n","        # predict on discriminator\n","        pred_concat = critic(feat_concat.view(-1, 512*2*2).detach())\n","\n","        # prepare real and fake label\n","        label_src = torch.ones(feat_src.size(0)).long().cuda() # source = 1\n","        label_tgt = torch.zeros(feat_tgt.size(0)).long().cuda() # target = 1\n","        label_concat = torch.cat((label_src, label_tgt), dim=0)\n","\n","        # compute loss for critic\n","        loss_critic = criterion(pred_concat, label_concat)\n","        loss_critic.backward()\n","\n","        # optimize critic\n","        optimizer_critic.step()\n","        pred_cls = torch.squeeze(pred_concat.max(1)[1])\n","        domain_acc += (pred_cls == label_concat).float().mean()\n","\n","        ############################\n","        # 2.2 train target encoder #\n","        ############################\n","\n","        # zero gradients for optimizer\n","        optimizer_critic.zero_grad()\n","        optimizer_tgt.zero_grad()\n","\n","        # extract and target features\n","        feat_tgt = tgt_encoder(images_tgt)\n","\n","        # predict on discriminator\n","        pred_tgt = critic(feat_tgt.view(-1, 512*2*2))\n","\n","        # prepare fake labels\n","        label_tgt = torch.ones(feat_tgt.size(0)).long().cuda()\n","\n","        # compute loss for target encoder\n","        loss_tgt = criterion(pred_tgt, label_tgt)\n","        loss_tgt.backward()\n","\n","        # optimize target encoder\n","        optimizer_tgt.step()\n","\n","    tgt_encoder.eval()\n","    src_classifier.eval()\n","    # init loss and accuracy\n","    target_val_loss = 0.0\n","    target_val_acc = 0.0\n","    # set loss function\n","    criterion = nn.CrossEntropyLoss()\n","    # evaluate network\n","    with torch.no_grad():\n","        target_val_loss = 0.0\n","        target_val_acc = 0.0\n","        for (images, labels) in tgt_data_loader_eval:\n","            images = images.cuda()\n","            labels = labels.cuda()\n","            preds = src_classifier(tgt_encoder(images).view(-1, 512*2*2))\n","            target_val_loss += criterion(preds, labels).item()\n","            target_val_acc += torch.sum(torch.argmax(preds, dim=1) == labels).item()\n","    if target_val_acc > best_val_acc:\n","        best_val_acc = target_val_acc\n","        torch.save(tgt_encoder.state_dict(), os.path.join(path, 'p4_USPS2MNISTM_tgt_encoder.pkl'))\n","        torch.save(src_classifier.state_dict(), os.path.join(path, 'p3_USPS2MNISTM_src_classifier.pkl'))\n","        print('save model')\n","\n","    print('target val acc: {:.4f}, target val loss: {:.5f}, domain acc: {:.5f}'.format(\n","        target_val_acc/len(tgt_data_loader_eval.dataset), target_val_loss/len(tgt_data_loader_eval), domain_acc/len_data_loader\n","    ))  "],"execution_count":37,"outputs":[{"output_type":"stream","text":["torchvision.__version__ = 0.5.0\n","torch.cuda.is_available() = True\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:100: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  input = module(input)\n"],"name":"stderr"},{"output_type":"stream","text":["save model\n","target val acc: 0.0924, target val loss: 2.31350, domain acc: 0.69585\n","target val acc: 0.0919, target val loss: 2.31511, domain acc: 0.73512\n","save model\n","target val acc: 0.0974, target val loss: 2.32059, domain acc: 0.90820\n","target val acc: 0.0924, target val loss: 2.31620, domain acc: 0.90191\n","save model\n","target val acc: 0.1010, target val loss: 2.31660, domain acc: 0.89079\n","target val acc: 0.0965, target val loss: 2.31292, domain acc: 0.95050\n"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-37-60300b340fd5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0mcritic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mdomain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages_src\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimages_tgt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc_data_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_data_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m         \u001b[0;31m###########################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;31m# 2.1 train discriminator #\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    383\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 385\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    386\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-6-1b3205f63d6c>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m             \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, pic)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m         \"\"\"\n\u001b[0;32m--> 136\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_pil_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36mto_pil_image\u001b[0;34m(pic, mode)\u001b[0m\n\u001b[1;32m    187\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Input type {} is not supported'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnpimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnpimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mfromarray\u001b[0;34m(obj, mode)\u001b[0m\n\u001b[1;32m   2739\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mversionadded\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1.1\u001b[0m\u001b[0;36m.6\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2740\u001b[0m     \"\"\"\n\u001b[0;32m-> 2741\u001b[0;31m     \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__array_interface__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2742\u001b[0m     \u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"shape\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2743\u001b[0m     \u001b[0mndim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"markdown","metadata":{"id":"RRyZg753Z-Ns"},"source":["#pretrain"]},{"cell_type":"code","metadata":{"id":"Xdt3FWcCZb86"},"source":["\"\"\"Pre-train encoder and classifier for source dataset.\"\"\"\n","\n","def train_src(encoder, classifier, src_train_data_loader, src_eval_data_loader, tgt_eval_data_loader):\n","    \"\"\"Train classifier for source domain.\"\"\"\n","    ####################\n","    # 1. setup network #\n","    ####################\n","\n","    # setup criterion and optimizer\n","    # optimizer = optim.Adam(list(encoder.parameters()) + list(classifier.parameters()), lr=1e-3, betas=(0.5, 0.9))\n","    optimizer_c = optim.Adam(classifier.parameters(), lr=1e-3)\n","    optimizer_e = optim.Adam(encoder.parameters(), lr=1e-3)\n","    criterion = nn.CrossEntropyLoss()\n","\n","    ####################\n","    # 2. train network #\n","    ####################\n","    num_epochs_pre = 100\n","    for epoch in range(num_epochs_pre):\n","        encoder.train()\n","        classifier.train()\n","        for step, (images, labels) in enumerate(src_train_data_loader):\n","            images = images.cuda()\n","            labels = labels.cuda()\n","            # zero gradients for optimizer\n","            optimizer_c.zero_grad()\n","            optimizer_e.zero_grad()\n","\n","            # compute loss for critic\n","            preds = classifier(encoder(images))\n","            loss = criterion(preds, labels)\n","\n","            # optimize source classifier\n","            loss.backward()\n","            optimizer_c.step()\n","            optimizer_e.step()\n","\n","        print(\"Epoch [{}/{}]: loss={}\".format(\n","                    epoch + 1, num_epochs_pre, loss.data.item()))\n","\n","        # eval model on test set\n","        eval_src(encoder, classifier, src_eval_data_loader, tgt_eval_data_loader)\n","\n","        # save model parameters\n","        if ((epoch + 1) % 20 == 0):\n","            save_model(encoder, \"ADDA-source-encoder-{}.pt\".format(epoch + 1))\n","            save_model(classifier, \"ADDA-source-classifier-{}.pt\".format(epoch + 1))\n","\n","    # # save final model\n","    save_model(encoder, \"ADDA-source-encoder-final.pt\")\n","    save_model(classifier,\"ADDA-source-classifier-final.pt\")\n","\n","    return encoder, classifier\n","\n","\n","def eval_src(encoder, classifier, src_eval_data_loader, tgt_eval_data_loader):\n","    \"\"\"Evaluate classifier for source domain.\"\"\"\n","    # set eval state for Dropout and BN layers\n","    encoder.eval()\n","    classifier.eval()\n","\n","    # init loss and accuracy\n","    loss_source = 0\n","    loss_target = 0\n","    acc_source = 0\n","    acc_target = 0\n","\n","    # set loss function\n","    criterion = nn.CrossEntropyLoss()\n","\n","    # evaluate network\n","    with torch.no_grad():\n","        for (images, labels) in src_eval_data_loader:\n","            images = images.cuda()\n","            labels = labels.cuda()\n","            preds = classifier(encoder(images))\n","            loss_source += criterion(preds, labels).data.item()\n","            acc_source += torch.sum(torch.argmax(preds, dim=1) == labels).item()\n","\n","        for (images, labels) in tgt_eval_data_loader:\n","            images = images.cuda()\n","            labels = labels.cuda()\n","            preds = classifier(encoder(images))\n","            loss_target += criterion(preds, labels).item()\n","            acc_target += torch.sum(torch.argmax(preds, dim=1) == labels).item()\n","\n","    print(\"Source Avg Loss = {}, Source Avg Accuracy = {:2%}\".format(\n","        loss_source/len(src_eval_data_loader), acc_source/len(src_eval_data_loader.dataset)))\n","    print(\"Target Avg Loss = {}, Target Avg Accuracy = {:2%}\".format(\n","        loss_target/len(tgt_eval_data_loader), acc_target/len(tgt_eval_data_loader.dataset)))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JnWvpwoQlFQa"},"source":["# ADDA\n"]},{"cell_type":"code","metadata":{"id":"edB5tVB4lIii"},"source":["def train_tgt(src_encoder, tgt_encoder, critic,\n","              src_data_loader, tgt_data_loader):\n","    \"\"\"Train encoder for target domain.\"\"\"\n","    ####################\n","    # 1. setup network #\n","    ####################\n","    # setup criterion and optimizer\n","    # criterion = nn.CrossEntropyLoss()\n","    criterion = nn.BCEWithLogitsLoss()\n","    optimizer_tgt = optim.Adam(tgt_encoder.parameters(), lr=1e-4, betas=(0.5,0.9))\n","    optimizer_critic = optim.Adam(critic.parameters(), lr=1e-4, betas=(0.5,0.9))\n","    len_data_loader = min(len(src_data_loader), len(tgt_data_loader))\n","\n","    ####################\n","    # 2. train network #\n","    ####################\n","    num_epochs = 5\n","    for epoch in range(num_epochs):\n","        tgt_encoder.train()\n","        critic.train()\n","        # zip source and target data pair\n","        data_zip = enumerate(zip(src_data_loader, tgt_data_loader))\n","        for step, ((images_src, _), (images_tgt, _)) in data_zip:\n","            ###########################\n","            # 2.1 train discriminator #\n","            ###########################\n","            images = images.cuda()\n","            labels = labels.cuda()\n","            # zero gradients for optimizer\n","            optimizer_critic.zero_grad()\n","\n","            # extract and concat features\n","            feat_src = src_encoder(images_src)\n","            feat_tgt = tgt_encoder(images_tgt)\n","            feat_concat = torch.cat((feat_src, feat_tgt), 0)\n","\n","            # predict on discriminator\n","            pred_concat = critic(feat_concat.detach())\n","\n","            # prepare real and fake label\n","            label_src = torch.ones(feat_src.size(0)).long().cuda()\n","            label_tgt = torch.zeros(feat_tgt.size(0)).long().cuda()\n","            label_concat = torch.cat((label_src, label_tgt), 0)\n","\n","            # compute loss for critic\n","            loss_critic = criterion(pred_concat, label_concat)\n","            loss_critic.backward()\n","\n","            # optimize critic\n","            optimizer_critic.step()\n","\n","            pred_cls = torch.squeeze(pred_concat.max(1)[1])\n","            acc = (pred_cls == label_concat).float().mean()\n","\n","            ############################\n","            # 2.2 train target encoder #\n","            ############################\n","\n","            # zero gradients for optimizer\n","            optimizer_critic.zero_grad()\n","            optimizer_tgt.zero_grad()\n","\n","            # extract and target features\n","            feat_tgt = tgt_encoder(images_tgt)\n","\n","            # predict on discriminator\n","            pred_tgt = critic(feat_tgt)\n","\n","            # prepare fake labels\n","            label_tgt = torch.ones(feat_tgt.size(0)).long().cuda()\n","\n","            # compute loss for target encoder\n","            loss_tgt = criterion(pred_tgt, label_tgt)\n","            loss_tgt.backward()\n","\n","            # optimize target encoder\n","            optimizer_tgt.step()\n","\n","            #######################\n","            # 2.3 print step info #\n","            #######################\n","            if ((step + 1) % 50 == 0):\n","                print(\"Epoch [{}/{}] Step [{}/{}]: d_loss={:.5f} g_loss={:.5f} acc={:.5f}\".format(\n","                          epoch + 1, num_epochs, step + 1, len_data_loader, loss_critic.item(), loss_tgt.item(), acc.item()))\n","\n","        #############################\n","        # 2.4 save model parameters #\n","        #############################\n","        if ((epoch + 1) % 50 == 0):\n","            torch.save(critic.state_dict(), os.path.join(model_path, \"ADDA-critic-{}.pt\".format(epoch + 1)))\n","            torch.save(tgt_encoder.state_dict(), os.path.join(model_path, \"ADDA-target-encoder-{}.pt\".format(epoch + 1)))\n","            print('save model')\n","\n","    torch.save(critic.state_dict(), os.path.join(model_path, \"ADDA-critic-final.pt\"))\n","    torch.save(tgt_encoder.state_dict(), os.path.join(model_path, \"ADDA-target-encoder-final.pt\"))\n","    return tgt_encoder\n","\n","def eval_tgt(encoder, classifier, data_loader):\n","    \"\"\"Evaluation for target encoder by source classifier on target dataset.\"\"\"\n","    # set eval state for Dropout and BN layers\n","    encoder.eval()\n","    classifier.eval()\n","\n","    # init loss and accuracy\n","    loss = 0\n","    acc = 0\n","\n","    # set loss function\n","    criterion = nn.CrossEntropyLoss()\n","\n","    # evaluate network\n","    with torch.no_grad():\n","        for (images, labels) in data_loader:\n","            images = images.cuda()\n","            labels = labels.cuda()\n","\n","            preds = classifier(encoder(images))\n","            loss += criterion(preds, labels).item()\n","            acc += torch.sum(torch.argmax(preds, dim=1) == labels).item()\n","    print(\"Avg Loss = {}, Avg Accuracy = {}\".format(loss/len(data_loader), acc/len(data_loader.dataset)))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"bLyMrL1Xa_Yb","executionInfo":{"status":"error","timestamp":1606422409151,"user_tz":-480,"elapsed":1553388,"user":{"displayName":"曹林熹","photoUrl":"","userId":"17830086974482193998"}},"outputId":"a12049f1-fb81-4b40-da72-224f269f9ed6"},"source":["#usps_mnistm\n","import torch.backends.cudnn as cudnn\n","def save_model(net, filename):\n","    \"\"\"Save trained model.\"\"\"\n","    if not os.path.exists(model_path):\n","        os.makedirs(model_path)\n","    torch.save(net.state_dict(),\n","               os.path.join(model_path, filename))\n","    print(\"save pretrained model to: {}\".format(os.path.join(model_path,filename)))\n","\n","if __name__ == '__main__':\n","    # init random seed\n","    same_seeds(0)\n","    model_path = './'\n","    # load dataset\n","    src_data_loader = usps_train_loader\n","    src_data_loader_eval = usps_test_loader\n","    tgt_data_loader = mnistm_train_loader\n","    tgt_data_loader_eval = mnistm_test_loader\n","\n","    # load models\n","    src_encoder = init_model(net=LeNetEncoder().cuda(), restore=os.path.join(model_path, \"ADDA-source-encoder-final.pkl\"))\n","    src_classifier = init_model(net=LeNetClassifier().cuda(), restore= os.path.join(model_path, \"ADDA-source-classifier-final.pkl\"))\n","    tgt_encoder = init_model(net=LeNetEncoder().cuda(), restore=os.path.join(model_path, \"tgt_encoder.pkl\"))\n","    critic = init_model(Discriminator(input_dims=500, hidden_dims=500, output_dims=2), restore=os.path.join(model_path, \"ADDA_discriminator.pkl\"))\n","\n","    # train source model\n","    print(\"=== Training classifier for source domain ===\")\n","    print(\">>> Source Encoder <<<\")\n","    print(src_encoder)\n","    print(\">>> Source Classifier <<<\")\n","    print(src_classifier)\n","    src_model_trained = True\n","    if not (src_encoder.restored and src_classifier.restored and src_model_trained):\n","        src_encoder, src_classifier = train_src(src_encoder, src_classifier, src_data_loader, src_data_loader_eval, tgt_data_loader_eval)\n","\n","    # train target encoder by GAN\n","    print(\"=== Training encoder for target domain ===\")\n","    print(\">>> Target Encoder <<<\")\n","    print(tgt_encoder)\n","    print(\">>> Critic <<<\")\n","    print(critic)\n","\n","    # init weights of target encoder with those of source encoder\n","    if not tgt_encoder.restored:\n","        tgt_encoder.load_state_dict(src_encoder.state_dict())\n","    tgt_model_trained = True\n","    tgt_encoder_restore = \"snapshots/ADDA-target-encoder-final.pkl\"\n","    if not (tgt_encoder.restored and critic.restored and tgt_model_trained):\n","        tgt_encoder = train_tgt(src_encoder, tgt_encoder, critic, src_data_loader, tgt_data_loader)\n","\n","    # eval target encoder on test set of target dataset\n","    print(\"=== Evaluating classifier for encoded target domain ===\")\n","    print(\">>> source only <<<\")\n","    eval_tgt(src_encoder, src_classifier, tgt_data_loader_eval)\n","    print(\">>> domain adaption <<<\")\n","    eval_tgt(tgt_encoder, src_classifier, tgt_data_loader_eval)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["torchvision.__version__ = 0.5.0\n","torch.cuda.is_available() = True\n","=== Training classifier for source domain ===\n",">>> Source Encoder <<<\n","LeNetEncoder(\n","  (encoder): Sequential(\n","    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(3, 3))\n","    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (2): ReLU()\n","    (3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (5): ReLU()\n","    (6): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (7): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (8): ReLU()\n","    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    (10): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (11): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (12): ReLU()\n","    (13): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    (14): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (15): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (16): ReLU()\n","    (17): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    (18): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (19): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (20): ReLU()\n","    (21): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","  )\n",")\n",">>> Source Classifier <<<\n","LeNetClassifier(\n","  (fc2): Sequential(\n","    (0): Linear(in_features=2048, out_features=1024, bias=True)\n","    (1): ReLU(inplace=True)\n","    (2): Dropout(p=0.5, inplace=False)\n","    (3): Linear(in_features=1024, out_features=512, bias=True)\n","    (4): ReLU(inplace=True)\n","    (5): Dropout(p=0.5, inplace=False)\n","    (6): Linear(in_features=512, out_features=10, bias=True)\n","  )\n",")\n","Epoch [1/100]: loss=0.143842875957489\n","Source Avg Loss = 0.5431864117272198, Source Avg Accuracy = 82.909816%\n","Target Avg Loss = 4.216171735411237, Target Avg Accuracy = 15.370000%\n","Epoch [2/100]: loss=0.9312077164649963\n","Source Avg Loss = 0.45953506464138627, Source Avg Accuracy = 88.241156%\n","Target Avg Loss = 4.921455035543746, Target Avg Accuracy = 15.180000%\n","Epoch [3/100]: loss=0.13934928178787231\n","Source Avg Loss = 0.26554674468934536, Source Avg Accuracy = 91.928251%\n","Target Avg Loss = 10.809510856677013, Target Avg Accuracy = 14.130000%\n","Epoch [4/100]: loss=0.6102273464202881\n","Source Avg Loss = 0.28509251319337636, Source Avg Accuracy = 91.131041%\n","Target Avg Loss = 5.691614518499678, Target Avg Accuracy = 16.930000%\n","Epoch [5/100]: loss=0.18573200702667236\n","Source Avg Loss = 0.32320367742795497, Source Avg Accuracy = 91.280518%\n","Target Avg Loss = 9.957751110101201, Target Avg Accuracy = 15.980000%\n","Epoch [6/100]: loss=0.0007386207580566406\n","Source Avg Loss = 0.23117701150476933, Source Avg Accuracy = 93.522671%\n","Target Avg Loss = 13.73210209038607, Target Avg Accuracy = 15.190000%\n","Epoch [7/100]: loss=0.013419091701507568\n","Source Avg Loss = 0.3713721511885524, Source Avg Accuracy = 90.782262%\n","Target Avg Loss = 7.21860624422693, Target Avg Accuracy = 16.570000%\n","Epoch [8/100]: loss=0.025264501571655273\n","Source Avg Loss = 0.18115239357575774, Source Avg Accuracy = 96.113602%\n","Target Avg Loss = 8.717441537577635, Target Avg Accuracy = 18.850000%\n","Epoch [9/100]: loss=0.0119820237159729\n","Source Avg Loss = 0.26535837817937136, Source Avg Accuracy = 94.270055%\n","Target Avg Loss = 16.178308547682064, Target Avg Accuracy = 15.000000%\n","Epoch [10/100]: loss=8.237361907958984e-05\n","Source Avg Loss = 0.25390234554652125, Source Avg Accuracy = 94.220229%\n","Target Avg Loss = 3.92537433326624, Target Avg Accuracy = 23.860000%\n","Epoch [11/100]: loss=0.007555663585662842\n","Source Avg Loss = 0.2587573134806007, Source Avg Accuracy = 94.020927%\n","Target Avg Loss = 12.739637429547159, Target Avg Accuracy = 15.540000%\n","Epoch [12/100]: loss=0.21818366646766663\n","Source Avg Loss = 0.20379776519257575, Source Avg Accuracy = 95.117090%\n","Target Avg Loss = 7.05597574999378, Target Avg Accuracy = 19.820000%\n","Epoch [13/100]: loss=0.21115976572036743\n","Source Avg Loss = 0.18938080256339163, Source Avg Accuracy = 95.067265%\n","Target Avg Loss = 5.23807622520787, Target Avg Accuracy = 22.960000%\n","Epoch [14/100]: loss=0.27477407455444336\n","Source Avg Loss = 0.2618734505958855, Source Avg Accuracy = 94.967613%\n","Target Avg Loss = 6.670703256206148, Target Avg Accuracy = 20.430000%\n","Epoch [15/100]: loss=0.017662018537521362\n","Source Avg Loss = 0.19757660117466003, Source Avg Accuracy = 94.917788%\n","Target Avg Loss = 4.5277142949924345, Target Avg Accuracy = 20.890000%\n","Epoch [16/100]: loss=0.030538350343704224\n","Source Avg Loss = 0.200096859713085, Source Avg Accuracy = 95.665172%\n","Target Avg Loss = 7.017754181175475, Target Avg Accuracy = 19.480000%\n","Epoch [17/100]: loss=0.23904937505722046\n","Source Avg Loss = 0.5313228284940124, Source Avg Accuracy = 93.223717%\n","Target Avg Loss = 10.448438528996364, Target Avg Accuracy = 17.750000%\n","Epoch [18/100]: loss=0.15884016454219818\n","Source Avg Loss = 0.18756910262163728, Source Avg Accuracy = 96.063777%\n","Target Avg Loss = 3.4553095504736446, Target Avg Accuracy = 28.720000%\n","Epoch [19/100]: loss=0.07256858050823212\n","Source Avg Loss = 0.2371867634356022, Source Avg Accuracy = 94.917788%\n","Target Avg Loss = 9.588240790518986, Target Avg Accuracy = 17.820000%\n","Epoch [20/100]: loss=0.00015437602996826172\n","Source Avg Loss = 0.22985220851842314, Source Avg Accuracy = 95.416044%\n","Target Avg Loss = 12.344211049899933, Target Avg Accuracy = 17.840000%\n","save pretrained model to: ./ADDA-source-encoder-20.pt\n","save pretrained model to: ./ADDA-source-classifier-20.pt\n","Epoch [21/100]: loss=0.5306097269058228\n","Source Avg Loss = 0.2497047649230808, Source Avg Accuracy = 94.668660%\n","Target Avg Loss = 13.040314255246692, Target Avg Accuracy = 18.340000%\n","Epoch [22/100]: loss=0.042315125465393066\n","Source Avg Loss = 0.22282004123553634, Source Avg Accuracy = 94.867962%\n","Target Avg Loss = 5.704749596346716, Target Avg Accuracy = 21.360000%\n","Epoch [23/100]: loss=0.007520079612731934\n","Source Avg Loss = 0.2866181757999584, Source Avg Accuracy = 95.216741%\n","Target Avg Loss = 8.639436782545344, Target Avg Accuracy = 20.190000%\n","Epoch [24/100]: loss=0.0016672015190124512\n","Source Avg Loss = 0.2602765467017889, Source Avg Accuracy = 94.668660%\n","Target Avg Loss = 11.0334870830463, Target Avg Accuracy = 18.980000%\n","Epoch [25/100]: loss=0.0002537965774536133\n","Source Avg Loss = 0.24405122571624815, Source Avg Accuracy = 95.366218%\n","Target Avg Loss = 6.167262068220005, Target Avg Accuracy = 24.390000%\n","Epoch [26/100]: loss=0.0001251697540283203\n","Source Avg Loss = 0.2072524527902715, Source Avg Accuracy = 95.914300%\n","Target Avg Loss = 6.060880979914574, Target Avg Accuracy = 22.780000%\n","Epoch [27/100]: loss=0.0008164048194885254\n","Source Avg Loss = 0.27412050298880786, Source Avg Accuracy = 95.017439%\n","Target Avg Loss = 8.769947085410926, Target Avg Accuracy = 24.110000%\n","Epoch [28/100]: loss=0.003309309482574463\n","Source Avg Loss = 0.2335447990335524, Source Avg Accuracy = 95.266567%\n","Target Avg Loss = 5.001452681365286, Target Avg Accuracy = 22.550000%\n","Epoch [29/100]: loss=0.00916212797164917\n","Source Avg Loss = 0.232544913131278, Source Avg Accuracy = 95.216741%\n","Target Avg Loss = 8.75056149549545, Target Avg Accuracy = 21.040000%\n","Epoch [30/100]: loss=0.015032023191452026\n","Source Avg Loss = 0.21286338969366625, Source Avg Accuracy = 95.166916%\n","Target Avg Loss = 8.4913693537378, Target Avg Accuracy = 20.180000%\n","Epoch [31/100]: loss=0.00013399124145507812\n","Source Avg Loss = 0.21332533948589116, Source Avg Accuracy = 95.665172%\n","Target Avg Loss = 16.229691347498804, Target Avg Accuracy = 17.250000%\n","Epoch [32/100]: loss=9.655952453613281e-06\n","Source Avg Loss = 0.23449144751066342, Source Avg Accuracy = 95.665172%\n","Target Avg Loss = 11.528792587814817, Target Avg Accuracy = 19.130000%\n","Epoch [33/100]: loss=0.00019550323486328125\n","Source Avg Loss = 0.30484403629088774, Source Avg Accuracy = 94.718485%\n","Target Avg Loss = 10.015129204768284, Target Avg Accuracy = 20.110000%\n","Epoch [34/100]: loss=1.5854835510253906e-05\n","Source Avg Loss = 0.28960621333681047, Source Avg Accuracy = 94.319880%\n","Target Avg Loss = 9.42584026846916, Target Avg Accuracy = 20.720000%\n","Epoch [35/100]: loss=0.31826597452163696\n","Source Avg Loss = 0.3340103490045294, Source Avg Accuracy = 94.768311%\n","Target Avg Loss = 22.110140029032518, Target Avg Accuracy = 16.740000%\n","Epoch [36/100]: loss=0.00036144256591796875\n","Source Avg Loss = 0.22414446098264307, Source Avg Accuracy = 96.462382%\n","Target Avg Loss = 14.596670357285031, Target Avg Accuracy = 17.470000%\n","Epoch [37/100]: loss=3.2186508178710938e-06\n","Source Avg Loss = 0.2640831231838092, Source Avg Accuracy = 95.565521%\n","Target Avg Loss = 14.748483202260012, Target Avg Accuracy = 17.540000%\n","Epoch [38/100]: loss=2.6226043701171875e-06\n","Source Avg Loss = 0.22841227147728205, Source Avg Accuracy = 96.013951%\n","Target Avg Loss = 7.905088485426204, Target Avg Accuracy = 21.410000%\n","Epoch [39/100]: loss=0.006533205509185791\n","Source Avg Loss = 0.24500717682531103, Source Avg Accuracy = 96.412556%\n","Target Avg Loss = 10.226362929981985, Target Avg Accuracy = 19.980000%\n","Epoch [40/100]: loss=0.041836559772491455\n","Source Avg Loss = 0.24665551737416536, Source Avg Accuracy = 95.565521%\n","Target Avg Loss = 11.67740966408116, Target Avg Accuracy = 19.900000%\n","save pretrained model to: ./ADDA-source-encoder-40.pt\n","save pretrained model to: ./ADDA-source-classifier-40.pt\n","Epoch [41/100]: loss=0.2534020245075226\n","Source Avg Loss = 0.20003908779472113, Source Avg Accuracy = 97.010463%\n","Target Avg Loss = 8.014327744769442, Target Avg Accuracy = 22.670000%\n","Epoch [42/100]: loss=0.0010570287704467773\n","Source Avg Loss = 0.29671148653142154, Source Avg Accuracy = 95.166916%\n","Target Avg Loss = 14.622832966458265, Target Avg Accuracy = 18.900000%\n","Epoch [43/100]: loss=0.01871761679649353\n","Source Avg Loss = 0.2644093739800155, Source Avg Accuracy = 95.964126%\n","Target Avg Loss = 22.48210059001947, Target Avg Accuracy = 16.400000%\n","Epoch [44/100]: loss=0.8715997934341431\n","Source Avg Loss = 0.2975347681203857, Source Avg Accuracy = 95.316393%\n","Target Avg Loss = 8.735351605020512, Target Avg Accuracy = 23.400000%\n","Epoch [45/100]: loss=2.9096312522888184\n","Source Avg Loss = 0.21790751913795248, Source Avg Accuracy = 95.864474%\n","Target Avg Loss = 13.094818619406148, Target Avg Accuracy = 19.820000%\n","Epoch [46/100]: loss=0.016839414834976196\n","Source Avg Loss = 0.2640413671033457, Source Avg Accuracy = 95.266567%\n","Target Avg Loss = 12.62739339451881, Target Avg Accuracy = 18.430000%\n","Epoch [47/100]: loss=0.0009513497352600098\n","Source Avg Loss = 0.24570887500885874, Source Avg Accuracy = 95.465869%\n","Target Avg Loss = 9.93884118195552, Target Avg Accuracy = 20.340000%\n","Epoch [48/100]: loss=0.0\n","Source Avg Loss = 0.20947617891943082, Source Avg Accuracy = 96.163428%\n","Target Avg Loss = 9.323803585805711, Target Avg Accuracy = 21.020000%\n","Epoch [49/100]: loss=1.9431114196777344e-05\n","Source Avg Loss = 0.24262406857451424, Source Avg Accuracy = 96.312905%\n","Target Avg Loss = 9.4300151144623, Target Avg Accuracy = 20.890000%\n","Epoch [50/100]: loss=0.0009216666221618652\n","Source Avg Loss = 0.3011349079897627, Source Avg Accuracy = 95.814649%\n","Target Avg Loss = 14.301937771450941, Target Avg Accuracy = 19.480000%\n","Epoch [51/100]: loss=0.027922064065933228\n","Source Avg Loss = 0.23351269913837314, Source Avg Accuracy = 96.512207%\n","Target Avg Loss = 15.273889723856737, Target Avg Accuracy = 19.730000%\n","Epoch [52/100]: loss=0.006189227104187012\n","Source Avg Loss = 0.2592712742916774, Source Avg Accuracy = 95.615346%\n","Target Avg Loss = 6.34096051173605, Target Avg Accuracy = 23.380000%\n","Epoch [53/100]: loss=3.039836883544922e-05\n","Source Avg Loss = 0.28107806923799217, Source Avg Accuracy = 96.263079%\n","Target Avg Loss = 12.035264264246461, Target Avg Accuracy = 21.280000%\n","Epoch [54/100]: loss=6.330013275146484e-05\n","Source Avg Loss = 0.22390169190475717, Source Avg Accuracy = 96.562033%\n","Target Avg Loss = 13.890732601190068, Target Avg Accuracy = 19.330000%\n","Epoch [55/100]: loss=0.0\n","Source Avg Loss = 0.28845180780626833, Source Avg Accuracy = 96.163428%\n","Target Avg Loss = 11.435049208865804, Target Avg Accuracy = 20.290000%\n","Epoch [56/100]: loss=0.01408281922340393\n","Source Avg Loss = 0.2637204951606691, Source Avg Accuracy = 96.113602%\n","Target Avg Loss = 24.385666719667473, Target Avg Accuracy = 17.290000%\n","Epoch [57/100]: loss=0.40178248286247253\n","Source Avg Loss = 0.2442442575120367, Source Avg Accuracy = 96.811161%\n","Target Avg Loss = 3.1002294106088626, Target Avg Accuracy = 33.620000%\n","Epoch [58/100]: loss=0.019402623176574707\n","Source Avg Loss = 0.2212071564863436, Source Avg Accuracy = 96.163428%\n","Target Avg Loss = 10.612935023702633, Target Avg Accuracy = 21.130000%\n","Epoch [59/100]: loss=0.00025141239166259766\n","Source Avg Loss = 0.25681872823042795, Source Avg Accuracy = 96.063777%\n","Target Avg Loss = 5.908790442594297, Target Avg Accuracy = 25.510000%\n","Epoch [60/100]: loss=0.0030088424682617188\n","Source Avg Loss = 0.2726384324487299, Source Avg Accuracy = 96.013951%\n","Target Avg Loss = 10.604415817625204, Target Avg Accuracy = 22.070000%\n","save pretrained model to: ./ADDA-source-encoder-60.pt\n","save pretrained model to: ./ADDA-source-classifier-60.pt\n","Epoch [61/100]: loss=0.03933572769165039\n","Source Avg Loss = 0.26572595071047544, Source Avg Accuracy = 96.013951%\n","Target Avg Loss = 12.837564565573528, Target Avg Accuracy = 21.780000%\n","Epoch [62/100]: loss=0.0002963542938232422\n","Source Avg Loss = 0.3395472939591855, Source Avg Accuracy = 95.465869%\n","Target Avg Loss = 11.92335586790826, Target Avg Accuracy = 21.770000%\n","Epoch [63/100]: loss=0.00057220458984375\n","Source Avg Loss = 0.3523895834805444, Source Avg Accuracy = 95.465869%\n","Target Avg Loss = 22.502403077046583, Target Avg Accuracy = 17.960000%\n","Epoch [64/100]: loss=1.6689300537109375e-06\n","Source Avg Loss = 0.28255694085964933, Source Avg Accuracy = 95.964126%\n","Target Avg Loss = 14.941195409009412, Target Avg Accuracy = 19.310000%\n","Epoch [65/100]: loss=1.2278556823730469e-05\n","Source Avg Loss = 0.3040914998273365, Source Avg Accuracy = 95.515695%\n","Target Avg Loss = 15.303220074647552, Target Avg Accuracy = 19.060000%\n","Epoch [66/100]: loss=5.5909156799316406e-05\n","Source Avg Loss = 0.28798359888605773, Source Avg Accuracy = 95.964126%\n","Target Avg Loss = 21.22363473199735, Target Avg Accuracy = 17.300000%\n","Epoch [67/100]: loss=8.809566497802734e-05\n","Source Avg Loss = 0.2838067529373802, Source Avg Accuracy = 96.163428%\n","Target Avg Loss = 18.00674032709401, Target Avg Accuracy = 18.300000%\n","Epoch [68/100]: loss=2.4080276489257812e-05\n","Source Avg Loss = 0.3220590307028033, Source Avg Accuracy = 96.013951%\n","Target Avg Loss = 18.220473125482062, Target Avg Accuracy = 18.490000%\n","Epoch [69/100]: loss=0.03731013834476471\n","Source Avg Loss = 0.32714033947559074, Source Avg Accuracy = 95.665172%\n","Target Avg Loss = 23.81032115183059, Target Avg Accuracy = 17.450000%\n","Epoch [70/100]: loss=0.0009890198707580566\n","Source Avg Loss = 0.35772911563981324, Source Avg Accuracy = 96.063777%\n","Target Avg Loss = 19.026229591126654, Target Avg Accuracy = 18.830000%\n","Epoch [71/100]: loss=8.618831634521484e-05\n","Source Avg Loss = 0.3260053807753138, Source Avg Accuracy = 94.967613%\n","Target Avg Loss = 12.843091630631951, Target Avg Accuracy = 20.090000%\n","Epoch [72/100]: loss=3.3974647521972656e-06\n","Source Avg Loss = 0.33235735492780805, Source Avg Accuracy = 95.814649%\n","Target Avg Loss = 16.348291864820347, Target Avg Accuracy = 20.250000%\n","Epoch [73/100]: loss=0.00018978118896484375\n","Source Avg Loss = 0.29929738480132073, Source Avg Accuracy = 96.412556%\n","Target Avg Loss = 18.68961757611317, Target Avg Accuracy = 20.580000%\n","Epoch [74/100]: loss=0.09765046834945679\n","Source Avg Loss = 0.29343983047874644, Source Avg Accuracy = 96.163428%\n","Target Avg Loss = 16.567913025048128, Target Avg Accuracy = 19.180000%\n","Epoch [75/100]: loss=0.0005074143409729004\n","Source Avg Loss = 0.2902541284565814, Source Avg Accuracy = 96.113602%\n","Target Avg Loss = 19.846629962799657, Target Avg Accuracy = 18.530000%\n","Epoch [76/100]: loss=0.009695470333099365\n","Source Avg Loss = 0.3237849557772279, Source Avg Accuracy = 95.515695%\n","Target Avg Loss = 28.925963177043162, Target Avg Accuracy = 18.180000%\n","Epoch [77/100]: loss=0.0057582855224609375\n","Source Avg Loss = 0.3183545686188154, Source Avg Accuracy = 95.964126%\n","Target Avg Loss = 25.376517156127154, Target Avg Accuracy = 18.720000%\n","Epoch [78/100]: loss=0.001552492380142212\n","Source Avg Loss = 0.3219869913882576, Source Avg Accuracy = 96.113602%\n","Target Avg Loss = 15.060590051541663, Target Avg Accuracy = 21.810000%\n","Epoch [79/100]: loss=0.000478208065032959\n","Source Avg Loss = 0.30588142335182056, Source Avg Accuracy = 95.665172%\n","Target Avg Loss = 14.588065147399902, Target Avg Accuracy = 21.860000%\n","Epoch [80/100]: loss=0.015338212251663208\n","Source Avg Loss = 0.2833170164376497, Source Avg Accuracy = 96.013951%\n","Target Avg Loss = 16.05406538240469, Target Avg Accuracy = 20.150000%\n","save pretrained model to: ./ADDA-source-encoder-80.pt\n","save pretrained model to: ./ADDA-source-classifier-80.pt\n","Epoch [81/100]: loss=5.960464477539062e-07\n","Source Avg Loss = 0.3443645756924525, Source Avg Accuracy = 95.814649%\n","Target Avg Loss = 21.304691326845983, Target Avg Accuracy = 19.770000%\n","Epoch [82/100]: loss=0.03279334306716919\n","Source Avg Loss = 0.29417823906987906, Source Avg Accuracy = 95.465869%\n","Target Avg Loss = 30.0220052512588, Target Avg Accuracy = 18.050000%\n","Epoch [83/100]: loss=1.71661376953125e-05\n","Source Avg Loss = 0.27085522253764793, Source Avg Accuracy = 96.063777%\n","Target Avg Loss = 13.609135645969658, Target Avg Accuracy = 20.500000%\n","Epoch [84/100]: loss=0.0017600059509277344\n","Source Avg Loss = 0.2618131795316003, Source Avg Accuracy = 96.412556%\n","Target Avg Loss = 16.090741400506086, Target Avg Accuracy = 21.730000%\n","Epoch [85/100]: loss=1.1324882507324219e-05\n","Source Avg Loss = 0.31695147848222405, Source Avg Accuracy = 96.113602%\n","Target Avg Loss = 13.704688132948176, Target Avg Accuracy = 22.750000%\n","Epoch [86/100]: loss=4.082918167114258e-05\n","Source Avg Loss = 0.28046564833493903, Source Avg Accuracy = 96.512207%\n","Target Avg Loss = 15.691295168202394, Target Avg Accuracy = 21.520000%\n","Epoch [87/100]: loss=2.384185791015625e-07\n","Source Avg Loss = 0.32199741509975865, Source Avg Accuracy = 96.562033%\n","Target Avg Loss = 18.022172496576978, Target Avg Accuracy = 20.020000%\n","Epoch [88/100]: loss=9.5367431640625e-06\n","Source Avg Loss = 0.31330553023144603, Source Avg Accuracy = 96.512207%\n","Target Avg Loss = 19.487868302946637, Target Avg Accuracy = 18.990000%\n","Epoch [89/100]: loss=0.0001360177993774414\n","Source Avg Loss = 0.4119272814132273, Source Avg Accuracy = 96.063777%\n","Target Avg Loss = 24.26774135516707, Target Avg Accuracy = 18.210000%\n","Epoch [90/100]: loss=0.08506631851196289\n","Source Avg Loss = 0.3403486346360296, Source Avg Accuracy = 95.914300%\n","Target Avg Loss = 9.317882911414857, Target Avg Accuracy = 23.180000%\n","Epoch [91/100]: loss=0.002773106098175049\n","Source Avg Loss = 0.3202422456815839, Source Avg Accuracy = 96.113602%\n","Target Avg Loss = 18.992380318368316, Target Avg Accuracy = 19.850000%\n","Epoch [92/100]: loss=9.5367431640625e-07\n","Source Avg Loss = 0.33356578822713345, Source Avg Accuracy = 96.213254%\n","Target Avg Loss = 17.66022815218397, Target Avg Accuracy = 20.350000%\n","Epoch [93/100]: loss=0.00628429651260376\n","Source Avg Loss = 0.3542764076264575, Source Avg Accuracy = 96.163428%\n","Target Avg Loss = 33.83130729578103, Target Avg Accuracy = 18.170000%\n","Epoch [94/100]: loss=8.940696716308594e-06\n","Source Avg Loss = 0.3281328280281741, Source Avg Accuracy = 96.213254%\n","Target Avg Loss = 31.628999843718898, Target Avg Accuracy = 19.320000%\n","Epoch [95/100]: loss=0.00438535213470459\n","Source Avg Loss = 0.37132109847152606, Source Avg Accuracy = 95.665172%\n","Target Avg Loss = 33.894163872785626, Target Avg Accuracy = 18.540000%\n","Epoch [96/100]: loss=0.0\n","Source Avg Loss = 0.32771184854209423, Source Avg Accuracy = 96.013951%\n","Target Avg Loss = 15.175416466536795, Target Avg Accuracy = 20.830000%\n","Epoch [97/100]: loss=9.5367431640625e-06\n","Source Avg Loss = 0.3684125828440301, Source Avg Accuracy = 95.814649%\n","Target Avg Loss = 15.156588742687443, Target Avg Accuracy = 21.280000%\n","Epoch [98/100]: loss=0.0\n","Source Avg Loss = 0.3120632069185376, Source Avg Accuracy = 96.412556%\n","Target Avg Loss = 18.604655927913203, Target Avg Accuracy = 19.960000%\n","Epoch [99/100]: loss=1.2874603271484375e-05\n","Source Avg Loss = 0.33483838487882167, Source Avg Accuracy = 95.515695%\n","Target Avg Loss = 17.577849631096907, Target Avg Accuracy = 17.180000%\n","Epoch [100/100]: loss=0.00012058019638061523\n","Source Avg Loss = 0.32518293434986845, Source Avg Accuracy = 95.714998%\n","Target Avg Loss = 14.369220788311807, Target Avg Accuracy = 20.350000%\n","save pretrained model to: ./ADDA-source-encoder-100.pt\n","save pretrained model to: ./ADDA-source-classifier-100.pt\n","save pretrained model to: ./ADDA-source-encoder-final.pt\n","save pretrained model to: ./ADDA-source-classifier-final.pt\n","=== Training encoder for target domain ===\n",">>> Target Encoder <<<\n","LeNetEncoder(\n","  (encoder): Sequential(\n","    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(3, 3))\n","    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (2): ReLU()\n","    (3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (5): ReLU()\n","    (6): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (7): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (8): ReLU()\n","    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    (10): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (11): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (12): ReLU()\n","    (13): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    (14): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (15): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (16): ReLU()\n","    (17): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    (18): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (19): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (20): ReLU()\n","    (21): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","  )\n",")\n",">>> Critic <<<\n","Discriminator(\n","  (layer): Sequential(\n","    (0): Linear(in_features=500, out_features=500, bias=True)\n","    (1): ReLU()\n","    (2): Linear(in_features=500, out_features=500, bias=True)\n","    (3): ReLU()\n","    (4): Linear(in_features=500, out_features=2, bias=True)\n","    (5): LogSoftmax()\n","  )\n",")\n"],"name":"stdout"},{"output_type":"error","ename":"UnboundLocalError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-11-b9674603f411>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0mtgt_encoder_restore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"snapshots/ADDA-target-encoder-final.pkl\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtgt_encoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestored\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mcritic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestored\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtgt_model_trained\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m         \u001b[0mtgt_encoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_tgt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc_encoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_encoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcritic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_data_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_data_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0;31m# eval target encoder on test set of target dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-10-7372f60da374>\u001b[0m in \u001b[0;36mtrain_tgt\u001b[0;34m(src_encoder, tgt_encoder, critic, src_data_loader, tgt_data_loader)\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0;31m# 2.1 train discriminator #\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;31m###########################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m             \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m             \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0;31m# zero gradients for optimizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'images' referenced before assignment"]}]}]}